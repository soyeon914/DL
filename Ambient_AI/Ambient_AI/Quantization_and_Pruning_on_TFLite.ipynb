{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cvRks32fRLNU1EjE0YAUjnViKMtnsu1l",
      "authorship_tag": "ABX9TyMLBkY/g5apWSndGEFf5Ik5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thdus5918647/DL/blob/main/Quantization_and_Pruning_on_TFLite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3hEWyeApu5W",
        "outputId": "29f16765-4602-4668-df9d-c0430c64fc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tensorflow Lite: Getting Started**\n",
        "\n",
        "First, download and normalize the fashion MNIST dataset"
      ],
      "metadata": {
        "id": "PVlQECVqTQGP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMYnwzftSx6L",
        "outputId": "3aabc706-2897-4c01-c134-033574222670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "Tcln9A3nS5DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the input image\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "test_images = test_images.astype(np.float32) / 255.0"
      ],
      "metadata": {
        "id": "BHwLQxVnS9Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(28,28)),\n",
        "    tf.keras.layers.Reshape(target_shape=(28,28,1)),\n",
        "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "# Compile and Train\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=10,\n",
        "    validation_data=(test_images, test_labels)\n",
        ")\n",
        "\n",
        "metrics = model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4Qh6R61Tgma",
        "outputId": "d4bf05ad-c240-4186-cf84-b7e639310623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 60s 31ms/step - loss: 0.4755 - accuracy: 0.8315 - val_loss: 0.3631 - val_accuracy: 0.8728\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3204 - accuracy: 0.8876 - val_loss: 0.3192 - val_accuracy: 0.8837\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.2808 - accuracy: 0.9005 - val_loss: 0.3090 - val_accuracy: 0.8913\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.2550 - accuracy: 0.9086 - val_loss: 0.2840 - val_accuracy: 0.9028\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2358 - accuracy: 0.9153 - val_loss: 0.2686 - val_accuracy: 0.9052\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2197 - accuracy: 0.9213 - val_loss: 0.2581 - val_accuracy: 0.9098\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2084 - accuracy: 0.9253 - val_loss: 0.2618 - val_accuracy: 0.9117\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.1978 - accuracy: 0.9296 - val_loss: 0.2521 - val_accuracy: 0.9119\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1893 - accuracy: 0.9311 - val_loss: 0.2688 - val_accuracy: 0.9077\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1805 - accuracy: 0.9348 - val_loss: 0.2561 - val_accuracy: 0.9125\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2561 - accuracy: 0.9125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model test loss: {metrics[0]:.03f} | test accuracy: {metrics[1]*100:.02f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehboFUzdU-Bp",
        "outputId": "dcbf1502-f3a9-4d45-ddbd-01603f9bd316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model test loss: 0.256 | test accuracy: 91.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습된 모델을 변환하기 위해서는 TensorFlow Lite converter라는 것을 이용해야 합니다. 이것은 TensorFlow 모델을 TensorFlow Lite 모델로 바꿔 줍니다."
      ],
      "metadata": {
        "id": "Ug_nSLZieBKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음과 같이 tflite 모델로 변환할 수 있습니다."
      ],
      "metadata": {
        "id": "V74YkKCseJe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert the model to TFLite, initialize a ***converter***"
      ],
      "metadata": {
        "id": "IgJFbk14lcSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting a tf.Keras model to a TensorFlow Lite model.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ygK7GveMn0",
        "outputId": "c0292023-db36-47b0-8176-231dd27d70f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "변화할 때 아무런 옵션을 주지 않았으므로 이 모델은 단순히 32-bit float tflite 모델입니다."
      ],
      "metadata": {
        "id": "4KLyPr_ClpF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now save the tflite model and deploy it on mobile!"
      ],
      "metadata": {
        "id": "qWU2f9SmlhpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "_mv-Gt9alMNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we have several options available to optimize the model\n",
        "- Typically, we use ***Tensorflow Model Optimization Toolkit***\n",
        "\n",
        "Two methods:\n",
        "- Quantization\n",
        "  - Post-Training Quantization(PTQ)\n",
        "  - Quantization-Aware Training(QAT)\n",
        "- Pruning <br/>\n",
        "\n",
        "다음 셀을 실행하면 quantization이 완료된다. "
      ],
      "metadata": {
        "id": "Fa3BviH4mOvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8 # or tf.int8\n",
        "converter.inference_output_type = tf.uint8 # or tf.int8\n",
        "\n",
        "post_quant_tflite_model = converter.convert()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCLV1rOFlRAJ",
        "outputId": "6c535a0d-73a9-43be-bd31-32772faa98d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate을 위한 helper function이다. input typer을 integer로 했으므로, quantize를 한 다음에 inference를 한다."
      ],
      "metadata": {
        "id": "PcLnuUzDuter"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  global test_images\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  input_index = input_details[\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" datset.\n",
        "  prediction_digits = []\n",
        "  for test_image in tqdm(test_images):\n",
        "    # quantize input\n",
        "    if input_details['dtype'] == np.uint8:\n",
        "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "      test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "    # pre-processing: add batch dimension\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print()\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy, output"
      ],
      "metadata": {
        "id": "Jbnp-fD_us7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tflite 모델로 inference를 하려면 **`Interpreter`**를 이용한다. **`model_content`** 대신에 **`model_path`**를 이용하면 tflite 파일을 불러올 수도 있다.\n",
        "<br/>\n",
        "inference를 하기 위해서는 <br/>\n",
        "interpreter 생성 -> **`allocate_tensor`** -> **`set_tensor`** -> **`invoke`** 순으로 실핸한다. <br/>\n",
        "지금 코드는 inference할 때 image를 하나씩 처리하는데, batch 단위로 처리하려면 r**`esize_tensor_input`**을 이용한다."
      ],
      "metadata": {
        "id": "JUrf_89UwK4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 32bit float tflite 모델로 interpreter 만듦\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "\n",
        "# inference할 때 input batch 수를 설정하는 법\n",
        "# input_details = interpreter.get_input_details()[0]\n",
        "# interpreter.resize_tensor_input(input_details[\"index\"], [10, 28, 28] )\n",
        "\n",
        "interpreter.allocate_tensors()\n",
        "tflite_test_accuracy, _ = evaluate_model(interpreter)\n",
        "\n",
        "# 8bit integer tflite\n",
        "post_quant_interpreter = tf.lite.Interpreter(model_content=post_quant_tflite_model)\n",
        "post_quant_interpreter.allocate_tensors()\n",
        "post_quant_tflite_test_accuracy, output = evaluate_model(post_quant_interpreter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcmPvIIEwKIp",
        "outputId": "d97a5704-62f9-4212-f953-e7c03241cd71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:04<00:00, 2449.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:01<00:00, 5332.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`get_input_details`**, **`get_output_details`**를 통해 input과 output의 type 및 quantization 정보를 알 수 있다."
      ],
      "metadata": {
        "id": "Hh2-1ewexwBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post_quant_interpreter.get_input_details()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEFPNLkDx9f7",
        "outputId": "c972fae1-2954-43c7-8b25-57a740fcb7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'serving_default_input_1:0',\n",
              "  'index': 0,\n",
              "  'shape': array([ 1, 28, 28], dtype=int32),\n",
              "  'shape_signature': array([-1, 28, 28], dtype=int32),\n",
              "  'dtype': numpy.uint8,\n",
              "  'quantization': (0.003921568859368563, 0),\n",
              "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
              "   'zero_points': array([0], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_quant_interpreter.get_output_details()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2lz5FjCyEGP",
        "outputId": "1e3e5c1f-8981-4b6e-da9e-014788556ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'StatefulPartitionedCall:0',\n",
              "  'index': 24,\n",
              "  'shape': array([ 1, 10], dtype=int32),\n",
              "  'shape_signature': array([-1, 10], dtype=int32),\n",
              "  'dtype': numpy.uint8,\n",
              "  'quantization': (0.00390625, 0),\n",
              "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
              "   'zero_points': array([0], dtype=int32),\n",
              "   'quantized_dimension': 0},\n",
              "  'sparsity_parameters': {}}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제로 inference output이 정수임을 확인할 수 있다."
      ],
      "metadata": {
        "id": "IasI2WKWyP4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTOtM_aMyG5o",
        "outputId": "bd8f43bc-c07e-4f34-f010-673a60175b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0 192   0  56   8   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "float 32 모델과 accuracy를 비교해 보자."
      ],
      "metadata": {
        "id": "ey0V-PdJyUiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('TFLite test accuracy: ', tflite_test_accuracy)\n",
        "print('Post-training Quant TFLite test_accuracy: ', post_quant_tflite_test_accuracy) # 비슷하거나 조금 떨어지는 것이 정상임"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_EXlfh9yL7G",
        "outputId": "fbe53d94-b7ea-43c3-80f4-0db990e50aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFLite test accuracy:  0.9125\n",
            "Post-training Quant TFLite test_accuracy:  0.9124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 파일로 저장해서 실제로 모델 크기가 줄어들었는지 확인해 보자."
      ],
      "metadata": {
        "id": "BFmmweWWykY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "# create temp file\n",
        "_, tflite_file = tempfile.mkstemp('.tflite')\n",
        "_, post_quant_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "# tflite model 저장\n",
        "with open(tflite_file, 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "with open(post_quant_tflite_file, 'wb') as f:\n",
        "  f.write(post_quant_tflite_model)\n",
        "\n",
        "tflite_size = os.path.getsize(tflite_file) / float(2**20)\n",
        "quantized_size = os.path.getsize(post_quant_tflite_file) / float(2**20)\n",
        "\n",
        "print(f\"Float model: {tflite_size:.03f} MB\")\n",
        "print(f\"Quantized model: {quantized_size:.03f} MB\")\n",
        "print(f\"Quantized model: {quantized_size/tflite_size:.02f} x Float model size\",)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wggP_zSuym_V",
        "outputId": "51f50e5d-4137-4aee-b29b-8bbcb253f2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float model: 0.082 MB\n",
            "Quantized model: 0.025 MB\n",
            "Quantized model: 0.31 x Float model size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **실습: TensorFlow에서 제공하는 pretrain model 가져와서 post-training quantization 해보기**\n",
        "\n",
        "TensorFlow에서는 여러 pretrained model을 제공한다. 아무거나 하나 가져와서 post-training quantization을 해보자."
      ],
      "metadata": {
        "id": "pX91LyXC0IrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrain 모델 import 하기.\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from PIL import Image\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "S4NKVAsy0V8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://cdn.education.com/files/526001_527000/526114/file_526114.jpg\"\n",
        "os.system(\"curl \" + url + \" > balloon.jpg\")\n",
        "img_path = 'balloon.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "display(img)\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x) # 각 모델을 학습할 때 preprocess했던 방법 사용"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "DplqQb3o0tJS",
        "outputId": "4472bfb6-20bd-4ab0-9028-9b4d6243633a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7FD1711F3C40>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate 할 때는 다음 함수를 이용"
      ],
      "metadata": {
        "id": "-UCUni8N1vQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_practice(interpreter, test_images):\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  input_index = input_details[\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for test_image in tqdm(test_images):\n",
        "    # quantize input\n",
        "    if input_details['dtype'] == np.uint8:\n",
        "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "      test_image = test_image / input_scale + input_zero_point\n",
        "    \n",
        "    # Pre-processing: add batch dimension\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    preds = interpreter.tensor(output_index)\n",
        "    preds = np.expand_dims(preds()[0], axis=0)\n",
        "\n",
        "  return preds"
      ],
      "metadata": {
        "id": "td6k1Ocu1ZqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. import한 pretrain 모델 가져오기\n",
        "imagenet = ResNet50(weights='imagenet')\n",
        "\n",
        "preds = imagenet.predict(x)\n",
        "# decode the results into a list of tuples (class, description, probability)\n",
        "print('Predicted: ', decode_predictions(preds, top=3)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCBI1Ron-uRa",
        "outputId": "d3693c08-5de7-405a-eac1-7aba11ae91ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Predicted:  [('n02782093', 'balloon', 0.9505768), ('n04023962', 'punching_bag', 0.03517213), ('n03544143', 'hourglass', 0.0015566874)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "크기 비교를 위한 32bit float tflite 모델"
      ],
      "metadata": {
        "id": "lbROZWcG_GQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(imagenet) # resnet50 tflite 변환 = 32bit float 형식\n",
        "imagenet_tflite = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtfODFDs_CPG",
        "outputId": "d0494f10-ab68-49c7-f345-c905db9104c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. full integer post-training quantization\n",
        "import numpy as np\n",
        "def representative_data_gen():\n",
        "  yield [x] # test할 이미지 1개만 사용\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(imagenet)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "post_quant_imagenet_tflite = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkq8j88B_Qog",
        "outputId": "c5050a1a-33e1-4963-eb77-bab2ae8bdbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. make interpreter and do inference\n",
        "# tflite 모델로 interpreter 만듦\n",
        "post_quant_imagenet_interpreter = tf.lite.Interpreter(model_content=post_quant_imagenet_tflite)\n",
        "\n",
        "post_quant_imagenet_interpreter.allocate_tensors()\n",
        "\n",
        "preds = evaluate_model_practice(post_quant_imagenet_interpreter, x)\n",
        "print('\\nPredicted: ', decode_predictions(preds, top=3)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzzHTkZHAFiB",
        "outputId": "a09e6f06-6a85-42f2-c3a8-25eb424cbeea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted:  [('n02782093', 'balloon', 245), ('n04023962', 'punching_bag', 7), ('n03942813', 'ping-pong_ball', 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure sizes of models.\n",
        "_, imagenet_tflite_file = tempfile.mkstemp('.tflite')\n",
        "_, post_quant_imagenet_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(tflite_file, 'wb') as f:\n",
        "  f.write(imagenet_tflite)\n",
        "\n",
        "with open(post_quant_tflite_file, 'wb') as f:\n",
        "  f.write(post_quant_imagenet_tflite)\n",
        "\n",
        "tflite_qat_size = os.path.getsize(tflite_file) / float(2**20)\n",
        "quantized_size = os.path.getsize(post_quant_tflite_file) / float(2**20)\n",
        "\n",
        "print(f\"Float model: {tflite_qat_size:.03f} MB\")\n",
        "print(f\"Quantized model: {quantized_size:.03f} MB\")\n",
        "print(f\"Quantized model: {quantized_size/tflite_qat_size:.02f} x Float model size\",)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTP4LY8TA6gg",
        "outputId": "72f1f884-fdff-4513-cd41-570b972a77b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float model: 97.430 MB\n",
            "Quantized model: 25.079 MB\n",
            "Quantized model: 0.26 x Float model size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Quantization Aware Training(QAT)** <br/>\n",
        "QAT는 나중에 quantization을 할 것이라는 것을 가정하고 학습하는 것이다. <br/>\n",
        "scratch부터 학습해도 되지만, 이미 학습된 모델을 fine tuning하는 것이 좋다고 한다. "
      ],
      "metadata": {
        "id": "LuJlR6MwCirO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "f6f8CTEkBo0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot"
      ],
      "metadata": {
        "id": "1fWrRnd1C-GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quantize whole model**<br/>\n",
        "Edge TPU와 같이 fully quantized 모델이 필요한 경우에는 전체 모델에 대해 QAT를 한다.<br/>\n",
        "위에서 학습한 float32모델로 QAT를 해볼 것이다.<br/>\n",
        "`quantize_model` 함수를 이용한다."
      ],
      "metadata": {
        "id": "DbhsUSYqDCT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
        "\n",
        "# 'quantize_model' requires a recompile.\n",
        "quant_aware_model.compile(optimizer='adam',\n",
        "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "quant_aware_model.summary() # 모든 layer가 quantize로 변하는 것을 확인할 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueyBUxh1DBZh",
        "outputId": "70d9c965-fab2-4e90-c975-5c61dfee3a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 28, 28)           3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_reshape (QuantizeWrap  (None, 28, 28, 1)        1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrapp  (None, 28, 28, 16)       195       \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quanti  (None, 14, 14, 16)       1         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWra  (None, 14, 14, 32)       4707      \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Quan  (None, 7, 7, 32)         1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (None, 1568)             1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 10)               15695     \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,604\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 114\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "layer에 quant가 붙은 것을 확인할 수 있다. </br>\n",
        "그 다음으로는, data의 일부를 이용해서 fine tuning한다."
      ],
      "metadata": {
        "id": "ha8CnuzZF6hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data 일부를 이용해서 fine tuning\n",
        "train_images_subset = train_images[0:1000] # out ot 60000\n",
        "train_labels_subset = train_labels[0:1000]\n",
        "\n",
        "quant_aware_model.fit(train_images, train_labels, batch_size=500, epochs=10, validation_split=0.1)\n",
        "model.fit(train_images, train_labels, batch_size=500, epochs=10, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbRDaPukEweX",
        "outputId": "2221d2e0-ac53-4035-c32d-cd670e45ca5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "108/108 [==============================] - 39s 340ms/step - loss: 0.1554 - accuracy: 0.9449 - val_loss: 0.1440 - val_accuracy: 0.9492\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 38s 357ms/step - loss: 0.1483 - accuracy: 0.9481 - val_loss: 0.1481 - val_accuracy: 0.9450\n",
            "Epoch 3/10\n",
            "108/108 [==============================] - 38s 351ms/step - loss: 0.1455 - accuracy: 0.9491 - val_loss: 0.1449 - val_accuracy: 0.9475\n",
            "Epoch 4/10\n",
            "108/108 [==============================] - 36s 337ms/step - loss: 0.1433 - accuracy: 0.9492 - val_loss: 0.1485 - val_accuracy: 0.9447\n",
            "Epoch 5/10\n",
            "108/108 [==============================] - 36s 335ms/step - loss: 0.1406 - accuracy: 0.9505 - val_loss: 0.1532 - val_accuracy: 0.9462\n",
            "Epoch 6/10\n",
            "108/108 [==============================] - 37s 338ms/step - loss: 0.1375 - accuracy: 0.9520 - val_loss: 0.1528 - val_accuracy: 0.9463\n",
            "Epoch 7/10\n",
            "108/108 [==============================] - 36s 336ms/step - loss: 0.1374 - accuracy: 0.9521 - val_loss: 0.1522 - val_accuracy: 0.9462\n",
            "Epoch 8/10\n",
            "108/108 [==============================] - 39s 360ms/step - loss: 0.1352 - accuracy: 0.9528 - val_loss: 0.1615 - val_accuracy: 0.9392\n",
            "Epoch 9/10\n",
            "108/108 [==============================] - 40s 370ms/step - loss: 0.1336 - accuracy: 0.9535 - val_loss: 0.1552 - val_accuracy: 0.9417\n",
            "Epoch 10/10\n",
            "108/108 [==============================] - 37s 340ms/step - loss: 0.1307 - accuracy: 0.9542 - val_loss: 0.1553 - val_accuracy: 0.9442\n",
            "Epoch 1/10\n",
            "108/108 [==============================] - 32s 292ms/step - loss: 0.1532 - accuracy: 0.9459 - val_loss: 0.1422 - val_accuracy: 0.9493\n",
            "Epoch 2/10\n",
            "108/108 [==============================] - 30s 281ms/step - loss: 0.1474 - accuracy: 0.9485 - val_loss: 0.1404 - val_accuracy: 0.9495\n",
            "Epoch 3/10\n",
            "108/108 [==============================] - 33s 301ms/step - loss: 0.1449 - accuracy: 0.9496 - val_loss: 0.1411 - val_accuracy: 0.9502\n",
            "Epoch 4/10\n",
            "108/108 [==============================] - 31s 284ms/step - loss: 0.1437 - accuracy: 0.9496 - val_loss: 0.1411 - val_accuracy: 0.9492\n",
            "Epoch 5/10\n",
            "108/108 [==============================] - 30s 279ms/step - loss: 0.1421 - accuracy: 0.9500 - val_loss: 0.1425 - val_accuracy: 0.9480\n",
            "Epoch 6/10\n",
            "108/108 [==============================] - 30s 279ms/step - loss: 0.1412 - accuracy: 0.9512 - val_loss: 0.1428 - val_accuracy: 0.9473\n",
            "Epoch 7/10\n",
            "108/108 [==============================] - 30s 279ms/step - loss: 0.1406 - accuracy: 0.9509 - val_loss: 0.1471 - val_accuracy: 0.9453\n",
            "Epoch 8/10\n",
            "108/108 [==============================] - 30s 279ms/step - loss: 0.1395 - accuracy: 0.9515 - val_loss: 0.1442 - val_accuracy: 0.9452\n",
            "Epoch 9/10\n",
            "108/108 [==============================] - 30s 281ms/step - loss: 0.1383 - accuracy: 0.9521 - val_loss: 0.1454 - val_accuracy: 0.9463\n",
            "Epoch 10/10\n",
            "108/108 [==============================] - 33s 301ms/step - loss: 0.1376 - accuracy: 0.9526 - val_loss: 0.1445 - val_accuracy: 0.9478\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd167efd2b0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아직 quantization을 하지 않았으므로 weight은 실수이다."
      ],
      "metadata": {
        "id": "hh66VPBeGmp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_aware_model.layers[2].get_weights()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_9NCw-2GWVb",
        "outputId": "a52cfc65-bacd-44fc-953c-677613ca6354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[[[ 3.20332319e-01, -4.10334051e-01,  2.16265187e-01,\n",
              "           -3.74895692e-01, -4.31641817e-01, -1.99177772e-01,\n",
              "           -3.91042471e-01,  4.38113570e-01,  3.69700909e-01,\n",
              "            3.63705941e-02,  1.00494556e-01,  5.29972911e-01,\n",
              "           -1.78167149e-01, -2.04178263e-02,  2.17097588e-02,\n",
              "           -1.02739464e-02]],\n",
              " \n",
              "         [[ 1.53704165e-02,  3.04935109e-02, -1.92629382e-01,\n",
              "            2.15839177e-01,  3.99846017e-01,  1.03315469e-02,\n",
              "            1.46673262e-01, -2.19875187e-01,  4.52047348e-01,\n",
              "           -4.75671262e-01,  2.60078646e-02, -3.45883548e-01,\n",
              "            4.23887849e-01,  1.79081619e-01, -1.77454129e-02,\n",
              "           -4.51611251e-01]],\n",
              " \n",
              "         [[-1.04765901e-02,  2.41397440e-01,  1.15572913e-02,\n",
              "            1.03852404e-02,  2.50991806e-02,  3.56819178e-03,\n",
              "            2.75381774e-01, -9.66981724e-02,  4.36247677e-01,\n",
              "            4.97107118e-01, -4.82433215e-02, -2.05285954e+00,\n",
              "           -4.07880902e-01, -7.31886327e-02,  1.94218650e-01,\n",
              "           -7.26611733e-01]]],\n",
              " \n",
              " \n",
              "        [[[ 5.67919970e-01,  1.61874354e-01,  4.67896491e-01,\n",
              "            1.57948099e-02,  2.05358058e-01, -6.98841084e-03,\n",
              "           -3.30321878e-01, -1.10117793e-01, -4.75393012e-02,\n",
              "           -1.73642859e-01,  1.45913824e-01,  2.03022406e-01,\n",
              "            6.88963309e-02, -5.81581414e-01, -5.73466644e-02,\n",
              "           -3.30365561e-02]],\n",
              " \n",
              "         [[-3.99865717e-01,  3.45862538e-01,  1.29488409e-02,\n",
              "            1.66411638e-01,  3.92821789e-01,  2.70754516e-01,\n",
              "            9.98252407e-02, -1.28562236e+00, -2.19241589e-01,\n",
              "           -2.90620238e-01,  2.42048845e-01, -4.16554034e-01,\n",
              "            3.26057792e-01,  4.21461314e-01, -6.51003063e-01,\n",
              "            4.17031109e-01]],\n",
              " \n",
              "         [[-1.23645678e-01, -1.12822026e-01, -4.95277196e-01,\n",
              "            3.92535478e-01, -3.51197481e-01, -6.40816391e-02,\n",
              "            2.03342035e-01, -2.66920543e+00, -7.99624324e-02,\n",
              "            4.16860789e-01,  9.24750790e-02, -1.67584240e+00,\n",
              "           -3.89948189e-01,  1.77602917e-01, -5.81108689e-01,\n",
              "            3.12734872e-01]]],\n",
              " \n",
              " \n",
              "        [[[ 3.08886945e-01,  2.99133629e-01,  3.27563047e-01,\n",
              "            1.28290191e-01, -5.62621979e-04,  4.99735057e-01,\n",
              "           -2.98428804e-01,  1.12876311e-01, -3.20878476e-01,\n",
              "           -1.28815368e-01, -9.41009894e-02, -6.58778176e-02,\n",
              "            5.03582545e-02, -3.82949114e-01,  5.28677881e-01,\n",
              "            1.08969986e-01]],\n",
              " \n",
              "         [[-4.90637928e-01, -2.65878528e-01,  2.61391103e-01,\n",
              "           -3.79763186e-01,  2.32108429e-01,  1.90719441e-01,\n",
              "            3.35121989e-01, -1.03776705e+00, -2.37301722e-01,\n",
              "            9.21950340e-02,  2.06069738e-01,  3.34581107e-01,\n",
              "            2.16359690e-01,  2.00557530e-01,  4.78392929e-01,\n",
              "            1.65331781e-01]],\n",
              " \n",
              "         [[-1.99652854e-02, -1.18816584e-01, -5.89346647e-01,\n",
              "           -1.10501304e-01, -4.63719964e-01, -1.64767727e-01,\n",
              "           -1.37526849e-02, -2.33784294e+00, -3.11879963e-01,\n",
              "            2.36310810e-02,  4.53840494e-02,  1.58399805e-01,\n",
              "           -4.00736555e-02,  1.16307266e-01,  1.10555626e-01,\n",
              "            4.33279946e-02]]]], dtype=float32),\n",
              " array([-0.06262971, -0.02240362, -0.01857162,  0.01064464,  0.00361225,\n",
              "        -0.02458765, -0.00113099,  0.3370007 ,  0.00234229,  0.0061507 ,\n",
              "        -0.00334869,  0.14488277,  0.00481558, -0.02204199,  0.00138938,\n",
              "         0.14196594], dtype=float32),\n",
              " -1,\n",
              " array([-0.56785893, -0.41037723, -0.5893405 , -0.39251658, -0.4636995 ,\n",
              "        -0.499551  , -0.39100173, -2.668592  , -0.45186198, -0.49707085,\n",
              "        -0.24189168, -2.0524588 , -0.42359442, -0.58155507, -0.65126777,\n",
              "        -0.72665495], dtype=float32),\n",
              " array([0.56785893, 0.41037723, 0.5893405 , 0.39251658, 0.4636995 ,\n",
              "        0.499551  , 0.39100173, 2.668592  , 0.45186198, 0.49707085,\n",
              "        0.24189168, 2.0524588 , 0.42359442, 0.58155507, 0.65126777,\n",
              "        0.72665495], dtype=float32),\n",
              " -2.03647,\n",
              " 2.8546512]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실수 weight이므로 QAT를 한다고 해서 accuracy가 떨어지지 않는다."
      ],
      "metadata": {
        "id": "hM7-8ccgJz9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    test_images, test_labels, verbose=0)\n",
        "\n",
        "_, quant_aware_model_accuracy = quant_aware_model.evaluate(\n",
        "    test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('QAT test accuracy:', quant_aware_model_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq6rvNgnJYKX",
        "outputId": "427d0318-2c66-4600-9f73-a342ec23adb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy: 0.9185000061988831\n",
            "QAT test accuracy: 0.9182999730110168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 QAT로 학습한 모델을 quantization해보자.</br>\n",
        "post-training quantization과 비슷하게 TFLiteConverter를 이용하면 된다."
      ],
      "metadata": {
        "id": "2FsTiqsYKdCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8 # or tf.int8\n",
        "converter.inference_output_type = tf.uint8 # or tf.int8\n",
        "\n",
        "QAT_tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR5VTFMPKENJ",
        "outputId": "affd1716-4710-442f-c017-045db90f247e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as reshape_layer_call_fn, reshape_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op while saving (showing 5 of 12). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "quantization을 해도 accuracy가 떨어지지 않는 것을 알 수 있다."
      ],
      "metadata": {
        "id": "122riPXdLOjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QAT_interpreter = tf.lite.Interpreter(model_content=QAT_tflite_model)\n",
        "QAT_interpreter.allocate_tensors()\n",
        "\n",
        "QAT_test_accuracy, output = evaluate_model(QAT_interpreter)\n",
        "\n",
        "print('Baseline teset accuracy: ', baseline_model_accuracy) # 1\n",
        "print('Post-training Quantized test accuracy: ', post_quant_tflite_test_accuracy) # 3\n",
        "print('Quantization-Aware Training test accuracy: ', QAT_test_accuracy) # 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuUcE9p3K_KL",
        "outputId": "241d5d64-64ed-49ed-f438-0bff0ccaa16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:01<00:00, 5134.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Baseline teset accuracy:  0.9185000061988831\n",
            "Post-training Quantized test accuracy:  0.9124\n",
            "Quantization-Aware Training test accuracy:  0.9176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "마찬가지로 크기가 1/4로 줄어든 것을 알 수 있다."
      ],
      "metadata": {
        "id": "qplSRQHaLxHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create temp file\n",
        "_, QAT_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(QAT_tflite_file, 'wb') as f:\n",
        "  f.write(QAT_tflite_model)\n",
        "\n",
        "quantized_size = os.path.getsize(QAT_tflite_file) / float(2**20)\n",
        "\n",
        "print(f\"Float tflite model: {tflite_size:.03f} MB\")\n",
        "print(f\"Quantized QAT model: {quantized_size:.03f} MB\")\n",
        "print(f\"Quantized QAT model: {quantized_size/tflite_size:0.2f} x Float model size\",)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npHCViDBLr2c",
        "outputId": "3d230b86-d5ce-49b4-e52f-63aa590dbb45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float tflite model: 0.082 MB\n",
            "Quantized QAT model: 0.026 MB\n",
            "Quantized QAT model: 0.32 x Float model size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quantize some layers </br>**\n",
        "Edge TPU 등 특별한 하드웨어가 아닌 경우에는 선택적으로 layer를 quantization할 수도 있다. </br></br>\n",
        "**Tips for better accuracy** </br>\n",
        "- from scratch 보다는 fine tuning\n",
        "- 뒤쪽 layer를 quantization하기\n",
        "- 특별히 중요한 layer는 quantization 피하기 \n",
        "\n",
        "</br>\n",
        "\n",
        "`quantize_annotate_layer` 함수를 이용해서 특정 layer를 quantization할 것이라고 표시한다."
      ],
      "metadata": {
        "id": "wdfYCP0lkUyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers # 전체 layer 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSuIfunpmJXm",
        "outputId": "f95b1f63-af80-4ea9-c283-c70ac16ca19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.layers.reshaping.reshape.Reshape at 0x7fd178769580>,\n",
              " <keras.layers.convolutional.conv2d.Conv2D at 0x7fd178769730>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7fd178769700>,\n",
              " <keras.layers.convolutional.conv2d.Conv2D at 0x7fd1787697f0>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7fd178769790>,\n",
              " <keras.layers.reshaping.flatten.Flatten at 0x7fd178769e50>,\n",
              " <keras.layers.core.dense.Dense at 0x7fd177f00580>]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[-1].name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SOg5O63okP3P",
        "outputId": "a181ec4f-c9b9-47e0-8e47-1dea5f2715c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dense'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Function\n",
        "def apply_quantization_to_dense(layer):\n",
        "  # if isinstance(layer, tf.keras.layers.Dense)\n",
        "  if layer.name == 'dense':\n",
        "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "  else:\n",
        "    return layer"
      ],
      "metadata": {
        "id": "KLrF3Nh5la1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`clone_model` 함수와 helper function을 이용해서 annotated model을 만들고, `quantize_apply` 함수를 이용해서 QAT를 위한 모델을 만들면 된다."
      ],
      "metadata": {
        "id": "OlTkUYk6mHQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_model = tf.keras.models.clone_model(\n",
        "    model,\n",
        "    clone_function=apply_quantization_to_dense,\n",
        ")\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "quant_aware_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo5vE7hCl_cW",
        "outputId": "a684e485-6b7c-455b-fc3d-998fa6811290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 28, 28, 16)        160       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (None, 1568)             1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 10)               15695     \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,496\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 fine tuning이 아니라 처음부터 모델을 만드는 것이면 다음과 같이 모델을 만들면 된다.\n",
        "<br/><br/>\n",
        "\n",
        "**Sequential example**"
      ],
      "metadata": {
        "id": "FcxAiIyUmv26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10))    \n",
        "])\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "quant_aware_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY5aINHjmmaC",
        "outputId": "6cf5f112-a7e3-443f-aa0c-6739d22c2122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape_1 (Reshape)         (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 12)        120       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 13, 13, 12)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " quant_flatten_1 (QuantizeWr  (None, 2028)             1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_dense_1 (QuantizeWrap  (None, 10)               20295     \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,416\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functional example**"
      ],
      "metadata": {
        "id": "u0AfMurcng_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(28, 28))\n",
        "x = tf.keras.layers.Reshape(target_shape=(28, 28, 1))(inputs)\n",
        "x = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'))(x)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "outputs = tf.keras.layers.Dense(10)(x)\n",
        "\n",
        "annotated_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "quant_aware_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ0kp3kpndBQ",
        "outputId": "dbefe162-c1b3-4e2b-cb13-9e9725a125b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 28, 28)]          0         \n",
            "                                                                 \n",
            " quant_reshape_2 (QuantizeWr  (None, 28, 28, 1)        1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_conv2d_3 (QuantizeWra  (None, 26, 26, 12)       147       \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 13, 13, 12)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 2028)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                20290     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,438\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 28\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pruning** <br/>\n",
        "pruning은 불필요한(0에 가까운) weight을 0으로 만들어 없애면서 optimize를 한다. <br/>\n",
        "\n",
        "`tfmot.sparsity.keras.prune_low_magnitude`을 이용해서 모델을 만든다. 이 때, pruning 방식인 <br/>\n",
        "`tfmot.sparsity.keras.PolynomialDecay` 을 parameter로 넘겨준다. 다음과 같은 hyperparameter가 있다.\n",
        "- `initial_sparsity` : pruning을 시작할 때의 sparsity를 몇으로 할 지\n",
        "- `final_sparsity` : pruning을 끝낼 때의 sparsity를 몇으로 할 지\n",
        "- `begin_step` : pruning을 언제부터 진행할 지 (batch 단위의 step)\n",
        "- `end_step` : pruning을 언제 끝낼 지\n",
        "\n",
        "<br/>\n",
        "pruning을 하기 전에 baseline model을 저장하겠다. (크기 비교시 사용)"
      ],
      "metadata": {
        "id": "WUAZA4XtpEXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy: ', baseline_model_accuracy)\n",
        "\n",
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to: ', keras_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSSZQfpdoWCO",
        "outputId": "e21cd6d7-a447-4ed4-f87e-562db8f16e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy:  0.9185000061988831\n",
            "Saved baseline model to:  /tmp/tmpq_lhjvs8.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_model_optimization.python.core.sparsity.keras.prune import prune_low_magnitude\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set.\n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning.\n",
        "pruning_params = {\n",
        "    'pruning_schedule' : tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                              final_sparsity=0.80,\n",
        "                                                              begin_step=0,\n",
        "                                                              end_step=end_step)\n",
        "}\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params) # model 전체 pruning 적용 진행\n",
        "\n",
        "# 'prune_low_magnitude' requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu6NcpIV-LXn",
        "outputId": "4d52487b-8575-4c18-df65-53ef90e36466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_reshape  (None, 28, 28, 1)        1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d   (None, 28, 28, 16)       306       \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_max_poo  (None, 14, 14, 16)       1         \n",
            " ling2d (PruneLowMagnitude)                                      \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d_  (None, 14, 14, 32)       9250      \n",
            " 1 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_max_poo  (None, 7, 7, 32)         1         \n",
            " ling2d_1 (PruneLowMagnitude                                     \n",
            " )                                                               \n",
            "                                                                 \n",
            " prune_low_magnitude_flatten  (None, 1568)             1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_dense (  (None, 10)               31372     \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,932\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 20,442\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit을 할 때, `tfmot.sparsity.keras.UpdatePruningStep`을 callback으로 불러야 한다."
      ],
      "metadata": {
        "id": "7XQArQ3mMYft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(train_images, train_labels,\n",
        "                      batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
        "                      callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crfYHsLJLYD_",
        "outputId": "14d483b6-32e9-4e60-e47f-c9140ee5f5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "422/422 [==============================] - 42s 89ms/step - loss: 0.2492 - accuracy: 0.9096 - val_loss: 0.2809 - val_accuracy: 0.8958\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 34s 81ms/step - loss: 0.2705 - accuracy: 0.9033 - val_loss: 0.2605 - val_accuracy: 0.9018\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd172228c10>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "baseline 모델과 accuracy를 비교해 보자."
      ],
      "metadata": {
        "id": "teot3rexPDjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "    test_images, test_labels, verbose = 0)\n",
        "\n",
        "print('Baseline test accuracy : ', baseline_model_accuracy)\n",
        "print('Pruned test accuracy : ', model_for_pruning_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78hnAelMNYUY",
        "outputId": "58f41cf9-45d1-4121-8057-68f90ac117ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy :  0.9185000061988831\n",
            "Pruned test accuracy :  0.8945000171661377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pruned model의 크기가 줄어들었는 지 확인해 보자. <br/>\n",
        "\n",
        "위에 summary를 보면 pruning을 하기 위해 non-trainable parameter가 생긴 것을 알 수 있다. <br/>\n",
        "`tfmot.sparsity.keras.strip_pruning`을 이용해서 pruning에 사용한 variable을 제거해 준다."
      ],
      "metadata": {
        "id": "5UEVishaPVD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to: ', pruned_keras_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO0reo9TPUYs",
        "outputId": "69756936-561c-4379-8497-3535ddf13fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned Keras model to:  /tmp/tmpakp2pe4m.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tflite으로 저장"
      ],
      "metadata": {
        "id": "pOHwH6hsQCHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fTmHlTp6T74b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflie')\n",
        "\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "print('Saved pruned TFLite model to: ', pruned_tflite_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-9jurHMQA06",
        "outputId": "f397bcdd-efad-49fb-9f61-50541003f634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned TFLite model to:  /tmp/tmpgu5fwq4f.tflie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pruning을 해도 weight matrix의 크기는 그대로이다. 하지만 matrix의 값이 대부분 0이기 때문에 감소된 크기를 확인하려면 실제로 zip 파일 등으로 압축하는 과정을 거쳐야 한다."
      ],
      "metadata": {
        "id": "xTgXeXvaQZ8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bites.\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "DXoJ4H1DTnOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPFqFXVtQ0l1",
        "outputId": "d415b2cd-5737-4cbc-9da3-dd284cddfbc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of gzipped baseline Keras model: 78982.00 bytes\n",
            "Size of gzipped pruned Keras model: 26795.00 bytes\n",
            "Size of gzipped pruned TFlite model: 25489.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Combine pruning and quantization**\n",
        "\n",
        "pruning을 한 다음에 post-training quantization까지 하면 크기를 더 줄일 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "nN-QOVBRUHvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.util.nest import yield_flat_paths\n",
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)\n",
        "\n",
        "print('Saved quantized and pruned TFLite model to: ', quantized_and_pruned_tflite_file)\n",
        "\n",
        "print('Size of gzipped baseline Keras model: %.2f bytes' % (get_gzipped_model_size(keras_file)))\n",
        "print('Size of gzipped pruned and quantized TFlite model: %.2f bytes' % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))\n",
        "print(f\"Size of optimized model is {get_gzipped_model_size(quantized_and_pruned_tflite_file)/get_gzipped_model_size(keras_file)} of the baseline model\") # 12%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq5nvpOmUkgb",
        "outputId": "98abff7e-a44d-44f8-8879-a3695e82c839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved quantized and pruned TFLite model to:  /tmp/tmpr56qehal.tflite\n",
            "Size of gzipped baseline Keras model: 78982.00 bytes\n",
            "Size of gzipped pruned and quantized TFlite model: 9720.00 bytes\n",
            "Size of optimized model is 0.12306601504140184 of the baseline model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "baseline과 accuracy를 비교해 보자"
      ],
      "metadata": {
        "id": "hNWmJeHlYLAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy, _ = evaluate_model(interpreter)\n",
        "\n",
        "print('Baseline test accuracy: ', baseline_model_accuracy)\n",
        "print('Pruned and quantized TFLite test_accuracy: ', test_accuracy) # 모델 크기는 기존 모델의 12%밖에 안되는데, 성능은 많이 안떨어짐을 확인함."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2kbCembXT7x",
        "outputId": "4492d8b9-b1b6-4633-a3b7-4b926dc7e0dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:02<00:00, 3496.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Baseline test accuracy:  0.9185000061988831\n",
            "Pruned and quantized TFLite test_accuracy:  0.894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QAT와 비슷하게, pruning도 layer를 선택적으로 pruning할 수 있다."
      ],
      "metadata": {
        "id": "L9-XbtKfYlNK"
      }
    }
  ]
}