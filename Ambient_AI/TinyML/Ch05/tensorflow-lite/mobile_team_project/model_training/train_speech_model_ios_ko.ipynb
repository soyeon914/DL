{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iOS 어플리케이션 구동을 위한 커스텀 음성 인식 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금부터 간단한 음성 인식을 위한 20kb 정도의 iOS 어플리케이션용 TensorFlow Lite 모델을 훈련하는 방법을 살펴볼것입니다. 이 모델은 micro_speech 예제 어플리케이션에서 사용된 것과 같습니다.\n",
    "\n",
    "이 모델은 Google Colaboratory 환경에서도 사용할 수 있습니다.\n",
    "\n",
    "[Run in Google Colab](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train_speech_model_ios_ko.ipynb)\t[View source on GitHub](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train_speech_model_ios_ko.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북의 코드는 Python 스크립트를 실행하여 훈련을 진행한 뒤, 모델을 고정시킵니다. 그리고 Tensorflow Lite 모델 변환기(toco)를 통해 iOS 어플리케이션 구동을 위한 가벼운 모델을 생성합니다. \n",
    "\n",
    "**GPU 가속을 사용하면 훈련 속도가 훨씬 빨라집니다.** 훈련을 하기에 앞서, **Runtime -> Change runtime type**을 클릭하면 런타임을 **GPU** 환경으로 바꿀 수 있습니다. 훈련에는 GPU환경을 기준으로 18,000 Iteration의 경우 1시간 30분에서 2시간 정도 시간이 소요됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의존성/종속성 설치\n",
    "\n",
    "최신 버전 텐서플로우와 예제 코드가 작성될 당시 사용했던 텐서플로우의 버전이 다릅니다.\n",
    "이곳에서는 tesorflow 1.15 버전을 사용하여 훈련을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.15 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.15.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/sanghunkang/Library/Python/3.7/lib/python/site-packages (from tensorflow==1.15) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.18.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (3.12.4)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/sanghunkang/Library/Python/3.7/lib/python/site-packages (from tensorflow==1.15) (0.33.4)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (0.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/sanghunkang/Library/Python/3.7/lib/python/site-packages (from tensorflow==1.15) (1.11.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow==1.15) (1.31.0)\n",
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/sanghunkang/Lmibrary/Python/3.7/lib/python/site-packages (from protobuf>=3.6.1->tensorflow==1.15) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (0.15.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/yunho0130/tensorflow-lite.git \n",
    "!git clone https://github.com/sanghunkang/tensorflow-lite.git\n",
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab에서 실행중이라면, 왼쪽의 디렉토리에 tensorflow-lite가 추가된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 커스텀 데이터 생성 및 업로드\n",
    "\n",
    "훈련에 앞서서 훈련에 사용하고자 하는 데이터를 생성해야 합니다. speech_commands 예제에서는 대략 4,000개의 1초 길이의 16bit wav 파일을 훈련, 테스트 및 추론에 사용하고 있습니다. 클래스(키워드)별로 최소 2,000개 정도의 데이터를 준비하는 것이 좋습니다. 데이터의 개수와 품질은 모델의 성능에 중요한 영향을 줄 수 있습니다. \n",
    " \n",
    "ffmpeg 도구를 사용하면 변환작업을 빠르고 편리하게 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!for i in *.wav; do ffmpeg -y -i \"$i\" -acodec pcm_s16le -ac 1 -ar 16000 \"tmp/${i%.*}.wav\"; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당신이 만약 window 유저라면 다음 python 코드로 변환작업을 할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wave",
    "\n",
    "import glob",
    "\n",
    "def pcm2wav( pcm_file, wav_file, channels=1, bit_depth=16, sampling_rate=16000 ):",
    "\n",
    "# Check if the options are valid.",
    "\n",
    "if bit_depth % 8 != 0:",
    "\n",
    "    raise ValueError('bit_depth '+str(bit_depth)+' must be a multiple of 8.')",
    "\n",
    "with open( pcm_file, 'rb') as opened_pcm_file:",
    "\n",
    "    pcm_data = opened_pcm_file.read();",
    "\n",
    "    obj2write = wave.open( wav_file, 'wb')",
    "\n",
    "    obj2write.setnchannels( channels )",
    "\n",
    "    obj2write.setsampwidth( bit_depth // 8 )",
    "\n",
    "    obj2write.setframerate( sampling_rate )",
    "\n",
    "    obj2write.writeframes( pcm_data )",
    "\n",
    "    obj2write.close()",
    "\n",
    "file_list = glob.glob('file_directory/*.wav')",
    "\n",
    "for file in file_list:",
    "\n",
    "   pcm2wav(file, 'new_file_directory/'+file, 1, 16, 16000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환한 파일은 클래스별로 각각 폴더로 만들어서 저장한 후, 이 폴더들을 다시 하나의 디렉토리 안에 모아놓는 것이 좋습니다. \n",
    "우리는 이제 이 디렉토리를 데이터 디렉토리 경로로 지정하여 원하는 모델을 훈련할 수 있습니다.\n",
    "\n",
    "예를 들어, 당신이 총 4개의 키워드에 해당하는 데이터를 준비하였다고 한다면, 각 폴더명을 keyword_1, keyword_2, keyword_3, keyword_4로 설정한 후 키워드에 해당하는 파일을 넣어줍니다.\n",
    "위의 폴더 명들이 label로 자동으로 생성되는 것을 모델 훈련이 끝나면 확인할 수 있습니다.\n",
    "그 다음 이 keyword 폴더들을 한 폴더안에 모아줍니다. 이 폴더 이름을 keyword라고 한다면\n",
    "*--data_dir=keyword* 로 데이터 경로를 지정할 수 있습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련\n",
    "\n",
    "이제까지는 딥러닝에서 모두가 손 대기를 꺼려하는 작업이었습니다. 지금부터는 딥러닝에서 재미에 해당하는 부분입니다. 우리는 이제 speech_commands 예제에서 사용하는 모델의 아키텍쳐를 그대로 가져와서 그것을 우리만의 데이터셋에 적용할 것입니다. 다음의 커맨드를 실행하면 모델이 훈련될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/Resources/Python.app/Contents/MacOS/Python: can't open file 'tensorflow-lite/tensorflow/examples/speech_commands/train.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python3 tensorflow-lite/tensorflow/examples/speech_commands/train.py \\\n",
    "    --model_architecture=conv \\\n",
    "    --how_many_training_steps=10000,100  \\\n",
    "    --train_dir=./retrain_logs \\\n",
    "    --data_dir=data\n",
    "    --wanted_words=bulyiya,suzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "런타임이 모델을 훈련시키는 동안, 우리가 방금 실행시킨 커맨드가 어떤 작업을 하는지를 확인해 보겠습니다.\n",
    "\n",
    "- ```python3 tensorflow-lite/tensorflow/examples/speech_commands/train.py``` 훈련을 실행시키는 스크립트 파일\n",
    "- ```--model_architecture=conv``` 훈련할 모델 아키텍쳐. speech_commands 예제에서는 ```\"conv\"```라고 하는 모델 아키텍쳐를 사용하였습니다. 사용가능한 다른 모델 아키텍쳐들의 상세한 내용은 ```tensorflow-lite/tensorflow/examples/speech_commands/models.py```에서 확인할 수 있습니다.\n",
    "- ```--how_many_training_steps=10000,100``` 훈련을 반복할 횟수. 10000번의 가중치 업데이트 후 더 작은 learning-rate를 사용하여 100번의 추가 업데이트가 있을 것입니다.\n",
    "- ```--train_dir=./retrain_logs``` 훈련의 중간 결과 가중치들이 저장되는 경로. 이 결과파일들은 모델을 재훈련, 추론, 고정 및 tflite로 변환하는데 필요합니다.\n",
    "- ```--data_dir=data``` 앞에서 16bit .wav파일로 변환한 파일을 담고있는 폴더들의 상위폴더. 이 폴더는 bulyiya와 suzy 디렉토리를 하위 디렉토리로 가지고 있어야 합니다.\n",
    "- ```--wanted_words=bulyiya,suzy``` 인식하고자 하는 키워드들. ```--data_dir```에서 지정한 경로 안에 같은 이름으로 되어 훈련데이터를 가지고 있는 폴더가 각 키워드별로 있어야 합니다.\n",
    "\n",
    "만약 훈련을 중단한 모델을 중간부터 다시 훈련시키고자 한다면 ```--start_checkpoint=./retrain_logs/conv.ckpt-1100```와 같이 특정 체크포인트를 지정해서 그곳에서부터 다시 훈련을 시작할 수 있습니다.\n",
    "\n",
    "이 외에도 소스코드를 직접 수정하지 않고도 여러가지 조건을 변경하여 모델을 훈련시킬 수 있습니다. 이에 관한 상세한 내용은 ```tensorflow-lite/tensorflow/examples/speech_commands/train.py```의 FLAGS 입력 부분에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 고정\n",
    "\n",
    "오래 기다리셨습니다. 혹은 당신이 훌륭한 장비를 사용할 수 있는 축복받은 사람이라면 오래 기다리지 않았을 수도 있습니다. 이제 훈련의 결과로 생성된 체크포인트를 추론환경에서 사용하기 위하여 고정할 것입니다. 훈련 중간결과를 저장하기 위한 목적인 체크포인트 파일을 고정한 .pb파일은 체크포인트파일에서 훈련에 필요한 기능들을 제거하고 최적화하여 저장할 것입니다.\n",
    "\n",
    "(주: 2.0 version부터는 saved_model이라고 하는 api를 지원합니다. 기존의 pb로 만들어진 API보다 한단계 더 추상화가 된 API입니다. 불행히도, pb모델과 saved_model간의 변환은 자유롭지 못합니다. 그러나 다행인 것은 pb모델을 tflite모델로 변환하는 방법과 saved_model을 tflite모델로 변환하는 방법에는 크게 차이가 없다는 것입니다. 따라서 본 예제의 내용흐름을 잘 따라올 수 있으면 2.x version의 텐서플로우에서도 큰 어려움 없이 tflite모델을 생성하는 파이프라인을 구성할 수 있을 것입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sanghunkang/dev/2020-contributon\n",
      "2020-09-13 16:40:12.060728: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-09-13 16:40:12.083694: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbce90b07b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-09-13 16:40:12.083722: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "{\n",
      "  \"desired_samples\": 16000,\n",
      "  \"window_size_samples\": 480,\n",
      "  \"window_stride_samples\": 160,\n",
      "  \"spectrogram_length\": 98,\n",
      "  \"fingerprint_width\": 40,\n",
      "  \"fingerprint_size\": 3920,\n",
      "  \"label_count\": 4,\n",
      "  \"sample_rate\": 16000,\n",
      "  \"preprocess\": \"mfcc\",\n",
      "  \"average_window_width\": -1\n",
      "}\n",
      "DECODE>>> Tensor(\"decoded_sample_data_1d:0\", shape=(16000,), dtype=float32) DecodeWav(audio=<tf.Tensor 'decoded_sample_data:0' shape=(16000, 1) dtype=float32>, sample_rate=<tf.Tensor 'decoded_sample_data:1' shape=() dtype=int32>)\n",
      "\n",
      "\n",
      "\n",
      "DECODE>>> Tensor(\"decoded_sample_data_1d:0\", shape=(16000,), dtype=float32) DecodeWav(audio=<tf.Tensor 'decoded_sample_data:0' shape=(16000, 1) dtype=float32>, sample_rate=<tf.Tensor 'decoded_sample_data:1' shape=() dtype=int32>)\n",
      ">>> second_conv_shape: (1, 49, 20, 64)\n",
      "\n",
      "{'desired_samples': 16000, 'window_size_samples': 480, 'window_stride_samples': 160, 'spectrogram_length': 98, 'fingerprint_width': 40, 'fingerprint_size': 3920, 'label_count': 4, 'sample_rate': 16000, 'preprocess': 'mfcc', 'average_window_width': -1}\n",
      ">>>> Tensor(\"Reshape:0\", shape=(1, 3920), dtype=float32) Tensor(\"add_2:0\", shape=(1, 4), dtype=float32) 3920 Tensor(\"Mfcc:0\", shape=(1, 98, 40), dtype=float32)\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./content/retrain_logs/conv.ckpt-10100\n",
      "I0913 16:40:12.192130 4587834816 saver.py:1284] Restoring parameters from ./content/retrain_logs/conv.ckpt-10100\n",
      "WARNING:tensorflow:From tensorflow-lite/tensorflow/examples/speech_commands/freeze.py:191: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "W0913 16:40:12.221424 4587834816 deprecation.py:323] From tensorflow-lite/tensorflow/examples/speech_commands/freeze.py:191: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "<tf.Variable 'first_weights:0' shape=(20, 8, 1, 64) dtype=float32_ref>\n",
      "<tf.Variable 'first_bias:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'second_weights:0' shape=(20, 2, 64, 64) dtype=float32_ref>\n",
      "<tf.Variable 'second_bias:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'final_fc_weights:0' shape=(62720, 4) dtype=float32_ref>\n",
      "<tf.Variable 'final_fc_bias:0' shape=(4,) dtype=float32_ref>\n",
      "WARNING:tensorflow:From tensorflow-lite/tensorflow/examples/speech_commands/freeze.py:195: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W0913 16:40:12.223614 4587834816 deprecation.py:323] From tensorflow-lite/tensorflow/examples/speech_commands/freeze.py:195: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "W0913 16:40:12.223875 4587834816 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "I0913 16:40:12.235975 4587834816 graph_util_impl.py:334] Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n",
      "I0913 16:40:12.243736 4587834816 graph_util_impl.py:394] Converted 6 variables to const ops.\n",
      "INFO:tensorflow:Saved frozen graph to ./content/retrain_logs/conv.pb\n",
      "I0913 16:40:12.263437 4587834816 freeze.py:201] Saved frozen graph to ./content/retrain_logs/conv.pb\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/sanghunkang/dev/2020-contributon/\n",
    "!python3 tensorflow-lite/tensorflow/examples/speech_commands/freeze.py \\\n",
    "    --model_architecture=conv \\\n",
    "    --output_file=./content/retrain_logs/conv.pb \\\n",
    "    --start_checkpoint=./content/retrain_logs/conv.ckpt-10100 \\\n",
    "    --wanted_words=bulyiya,suzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "커맨드에서 사용한 FLAGS들의 세부 내용은 다음과 같습니다.\n",
    "- ```!python3 tensorflow-lite/tensorflow/examples/speech_commands/freeze.py``` 고정을 실행하는 스크립트\n",
    "- ```--model_architecture=conv``` 사용할 모델 아키텍쳐. 훈련에 사용한 것과 동일한 모델 아키텍쳐를 사용합니다.\n",
    "- ```--output_file=./content/retrain_logs/conv.pb``` 고정한 결과 .pb파일이 저장될 경로\n",
    "- ```--start_checkpoint=./content/retrain_logs/conv.ckpt-10100``` 고정시키고자 하는 중간훈련 버전. 체크포인트파일은 특별한 설정을 하지 않으면 일정한 단계마다 ```conv.ckpt-1000.data-00000-of-00001```, ```conv.ckpt-1000.index```, ```conv.ckpt-1000.meta``` 와 같은 파일들을 생성합니다. 훈련한 체크포인트 파일중 가장 퍼포먼스가 좋았던 버전을 지정하서 사용하면 될 것입니다.\n",
    "- ```--wanted_words=bulyiya,suzy``` 인식하고자 하는 키워드들. 훈련에 사용한 것과 동일한 키워드들을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프 탐색\n",
    "파이썬 메모리상이 아닌 일반적인(?) 명칭으로서 우리가 훈련한 모델의 각 부분이 어떻게 되는지를 알아볼 필요가 있습니다. 그래프를 탐색해서 알아낸 내부구조에 특정 부분을 우리는 tflite모델을 만들면서 input과 output으로 모델 외부와 상호작용할 수 있는 채널로 개방할 것입니다. 이 형식은 이후 iOS앱 개발에서 반드시 지켜줘야 하는 형식이므로 우리는 그 형식을 정확하게 iOS개발자에게 전달하기 위해서 모델의 그래프구조가 freezing 상태에서 어떻게 생겼는지 확인할 필요가 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"content/retrain_logs/conv.pb\"\n",
    "with tf.io.gfile.GFile(filename, 'rb') as f:\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    for n in tf.get_default_graph().as_graph_def().node:\n",
    "        print(n.name, n.op, n.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력결과는 텐서플로 그래프 상의 각 레이어의 이름, 레이어가 수행하는 연산, 레이어가 받는 입력들을 보여줍니다. 레이어가 받는 입력값이 iOS에서 입력으로 넣어주는 레이어일 것입니다. 이 부분은 우리의 예제에서는 ```decoded_sample_data```에 해당합니다. 따라서 .tflite 생성에 사용할 입력값 레이어는 ```--input_arrays=decoded_sample_data,decoded_sample_data:1```입니다. 마찬가지로 출력레이어로 사용할 레이어는 ```labels_softmax```입니다. 그러므로 .tflite 생성에 사용할 출력값 레이어는 ```--output_arrays=labels_softmax```입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .tflite 파일 생성\n",
    "\n",
    "그래프가 정의된 .pb파일을 interpreter로 컴파일 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-13 17:08:32.208072: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-09-13 17:08:32.274506: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbf52ec29b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-09-13 17:08:32.274545: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-09-13 17:08:32.308944: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n",
      "2020-09-13 17:08:32.309157: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2020-09-13 17:08:32.352659: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
      "2020-09-13 17:08:32.352699: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 31 nodes (-6), 30 edges (-6), time = 23.602ms.\n",
      "2020-09-13 17:08:32.352706: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 31 nodes (0), 30 edges (0), time = 7.156ms.\n"
     ]
    }
   ],
   "source": [
    "!toco --graph_def_file=./content/retrain_logs/conv.pb \\\n",
    "    --output_file=./content/retrain_logs/conv.tflite \\\n",
    "    --input_arrays=decoded_sample_data,decoded_sample_data:1 \\\n",
    "    --output_arrays=labels_softmax \\\n",
    "    --allow_custom_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "wav로 들어올 지 bufferStream으로 들어올지를 결정하는 것은 iOS 개발자의 결정사항이지만, 일단 input레이어에 약속된대로 데이터를 꽂고 나면 output레이어에서 모델의 추론 결과를 약속한대로 뱉어내야 합니다. 그것이 잘 되는지를 Python 코드 상에서 Interpreter와 예시 웨이브파일을 사용하여 확인할 것입니다. 우선 생성한 .tflite 파일이 그래프 구조, 그 중에서도 특히 입출력구조가 동일한지를 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'decoded_sample_data', 'index': 12, 'shape': array([16000,     1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'decoded_sample_data:1', 'index': 13, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0)}]\n",
      "{'name': 'AudioSpectrogram', 'index': 0, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'Conv2D_1_bias', 'index': 1, 'shape': array([64], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'Conv2D_bias', 'index': 2, 'shape': array([64], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'MatMul_bias', 'index': 3, 'shape': array([4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'MaxPool2d', 'index': 4, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'Mfcc', 'index': 5, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'Mfcc/sample_rate', 'index': 6, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'Relu', 'index': 7, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'Relu_1', 'index': 8, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'Reshape_1', 'index': 9, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'Reshape_1/shape', 'index': 10, 'shape': array([4], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'add_2', 'index': 11, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'decoded_sample_data', 'index': 12, 'shape': array([16000,     1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'decoded_sample_data:1', 'index': 13, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'final_fc_weights/read/transpose', 'index': 14, 'shape': array([    4, 62720], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'first_weights/read', 'index': 15, 'shape': array([64, 20,  8,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'labels_softmax', 'index': 16, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "{'name': 'second_weights/read', 'index': 17, 'shape': array([64, 20,  2, 64], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\n",
      "[{'name': 'decoded_sample_data', 'index': 12, 'shape': array([16000,     1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'decoded_sample_data:1', 'index': 13, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0)}]\n",
      "[{'name': 'labels_softmax', 'index': 16, 'shape': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"content/retrain_logs/conv.tflite\")\n",
    "print(interpreter.get_input_details())\n",
    "for tensor_detail in interpreter.get_tensor_details():\n",
    "    print(tensor_detail)\n",
    "\n",
    "print(interpreter.get_input_details())\n",
    "print(interpreter.get_output_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference [[2.4671772e-07 1.6052921e-04 9.9778628e-01 2.0529309e-03]]\n"
     ]
    }
   ],
   "source": [
    "# 인터프리터에 텐서들을 배정합니다. 이 작업을 통해서 데이터가 입력될 때 데이터가 계산되는 길이 정의됩니다.\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# 인식하고자 하는 웨이브파일을 읽어옵니다.\n",
    "samplerate, data = wavfile.read(\"data/bulyiya/불이야_test_1.wav\")\n",
    "\n",
    "# 첫 번쨰 input을 입력합니다. 그래프를 분석하면 (16000, 1)shape의 [0,1)의 np.float32임을 확인할 수 있습니다. 그것에 맞춰서 데이터를 수정합니다.\n",
    "input_data = np.array(data[:16000]/32767.0, dtype=np.float32).reshape((16000, 1))\n",
    "interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_data)\n",
    "# 두 번째 input을 입력합니다. 16it 웨이브 파일이므로 16000이어야 합니다.\n",
    "interpreter.set_tensor(interpreter.get_input_details()[1]['index'], np.int32(samplerate)) \n",
    "output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "이제 텐서들의 흐름 - 그러므로 텐서플로우(!) - 에 구체적인 데이터들이 채워졌습니다. 이제 이 텐서들의 흐름 끝에 우리가 최종적으로 얻고자 했던 인식결과가 나타나는지를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드 인식을 실제로 실행시킵니다.\n",
    "interpreter.invoke()\n",
    "\n",
    "# 실행결과를 읽어옵니다.\n",
    "print(\"inference\", interpreter.get_tensor(interpreter.get_output_details()[0]['index']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과는 길이가 4인 array입니다. 여기서 처음 두개의 값은 각각 silence와 unknown에 해당하는 값입니다. 마지막 두 개의 값 중 가장 높은 인식 결과가 입력한 파일의 분류와 같다면 커스텀 후 빌드한 모델은 훌륭하게 동작하고 있는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지 진행되었으면 이제 iOS에 배포할 모델의 준비는 끝났습니다. 모델이 준비되었다는 기쁜 소식을 iOS개발자에게 전달하고, 우리는 퇴근할 준비를 하도록 합시다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit5dea74c7e69948759ddf027ddea58c74"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}